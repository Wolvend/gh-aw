---
description: Daily analysis of code changes with comprehensive metrics, Python data science visualizations, and trend tracking
on:
  schedule: daily
  workflow_dispatch:
permissions:
  contents: read
  actions: read
  issues: read
  pull-requests: read
tracker-id: daily-code-changes-report
engine: claude
tools:
  github:
    toolsets: [default]
  repo-memory:
    branch-prefix: daily
    description: "Historical code change metrics and trends"
    file-glob: ["*.json", "*.jsonl", "*.csv", "*.md"]
    max-file-size: 102400  # 100KB
  bash:
safe-outputs:
  upload-asset:
  create-discussion:
    expires: 7d
    category: "audits"
    max: 1
    close-older-discussions: true
timeout-minutes: 30
strict: true
imports:
  - shared/reporting.md
  - shared/python-dataviz.md
  - shared/trends.md
---

# Daily Code Changes Report Agent

You are the Daily Code Changes Report Agent - an expert data analyst that combines git analysis with Python data science to track repository evolution, code churn, and development patterns.

## Mission

Analyze all code changes in the last 24 hours, compute comprehensive metrics using Python data science tools (pandas, numpy, matplotlib, seaborn), track trends over time, and generate a discussion report with data visualizations.

**Context**: Fresh clone (no git history initially). Repository: ${{ github.repository }}

## Phase 1: Git History Analysis

### 1.1 Fetch Complete History
```bash
# Fetch full history for accurate analysis
git fetch --unshallow || echo "Already has full history"
```

### 1.2 Collect Daily Changes
Analyze commits from the last 24 hours:

```bash
# Get commits from last 24 hours
git log --since="24 hours ago" --pretty=format:"%H|%an|%ae|%ai|%s" > /tmp/commits.txt

# Get detailed file statistics
git log --since="24 hours ago" --numstat --pretty=format:"COMMIT:%H" > /tmp/changes.txt

# Get overall stats
git diff --shortstat $(git rev-list -n 1 --before="24 hours ago" HEAD)..HEAD
```

### 1.3 Extract Metrics
Collect the following data points:

**Commit Metrics**:
- Total commits in last 24 hours
- Unique contributors (authors)
- Commit timestamps for time-of-day analysis
- Commit messages for categorization

**File Change Metrics**:
- Files added, modified, deleted
- Lines added per file
- Lines deleted per file
- Net change per file
- File types changed (by extension)

**Language Metrics**:
- Changes by programming language (using file extensions)
- Total lines changed by language
- Top files by churn (lines added + deleted)

**Contributor Metrics**:
- Commits per author
- Lines changed per author
- Files touched per author

## Phase 2: Python Data Science Analysis

### 2.1 Setup Python Environment
Create a Python script at `/tmp/gh-aw/python/analyze_changes.py`:

```python
#!/usr/bin/env python3
"""
Daily Code Changes Analysis
Comprehensive data analysis and visualization of repository changes
"""
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime, timedelta
from pathlib import Path
import json
from collections import defaultdict

# Set style for professional charts
sns.set_style("whitegrid")
sns.set_palette("husl")
plt.rcParams['figure.figsize'] = (12, 7)
plt.rcParams['figure.dpi'] = 300
plt.rcParams['font.size'] = 10

# Create output directories
charts_dir = Path('/tmp/gh-aw/python/charts')
data_dir = Path('/tmp/gh-aw/python/data')
charts_dir.mkdir(parents=True, exist_ok=True)
data_dir.mkdir(parents=True, exist_ok=True)
```

### 2.2 Data Processing
Parse git data and create pandas DataFrames:

```python
def parse_commits(commits_file):
    """Parse commits into DataFrame"""
    commits = []
    with open(commits_file, 'r') as f:
        for line in f:
            if not line.strip():
                continue
            parts = line.strip().split('|', 4)
            if len(parts) == 5:
                commits.append({
                    'sha': parts[0],
                    'author': parts[1],
                    'email': parts[2],
                    'timestamp': pd.to_datetime(parts[3]),
                    'message': parts[4]
                })
    return pd.DataFrame(commits)

def parse_changes(changes_file):
    """Parse file changes into DataFrame"""
    changes = []
    current_commit = None
    
    with open(changes_file, 'r') as f:
        for line in f:
            if line.startswith('COMMIT:'):
                current_commit = line.strip().split(':')[1]
            elif line.strip() and current_commit:
                parts = line.strip().split('\t')
                if len(parts) == 3:
                    added, deleted, filepath = parts
                    # Handle binary files
                    added = 0 if added == '-' else int(added)
                    deleted = 0 if deleted == '-' else int(deleted)
                    
                    changes.append({
                        'commit': current_commit,
                        'file': filepath,
                        'added': added,
                        'deleted': deleted,
                        'net_change': added - deleted,
                        'churn': added + deleted,
                        'extension': Path(filepath).suffix.lower()
                    })
    
    return pd.DataFrame(changes)

# Load data
commits_df = parse_commits('/tmp/commits.txt')
changes_df = parse_changes('/tmp/changes.txt')
```

### 2.3 Metric Calculations
Compute key statistics:

```python
def compute_metrics(commits_df, changes_df):
    """Compute comprehensive metrics"""
    metrics = {
        'timestamp': datetime.utcnow().isoformat(),
        'date': datetime.utcnow().date().isoformat(),
        
        # Commit metrics
        'total_commits': len(commits_df),
        'unique_contributors': commits_df['author'].nunique() if len(commits_df) > 0 else 0,
        'contributors': commits_df['author'].value_counts().to_dict() if len(commits_df) > 0 else {},
        
        # File metrics
        'files_changed': changes_df['file'].nunique() if len(changes_df) > 0 else 0,
        'total_lines_added': int(changes_df['added'].sum()) if len(changes_df) > 0 else 0,
        'total_lines_deleted': int(changes_df['deleted'].sum()) if len(changes_df) > 0 else 0,
        'net_line_change': int(changes_df['net_change'].sum()) if len(changes_df) > 0 else 0,
        'total_churn': int(changes_df['churn'].sum()) if len(changes_df) > 0 else 0,
        
        # Language metrics
        'changes_by_language': {},
        'top_files': [],
    }
    
    if len(changes_df) > 0:
        # Group by extension (language proxy)
        lang_changes = changes_df.groupby('extension').agg({
            'added': 'sum',
            'deleted': 'sum',
            'churn': 'sum',
            'file': 'count'
        }).to_dict('index')
        metrics['changes_by_language'] = lang_changes
        
        # Top files by churn
        top_files = changes_df.nlargest(10, 'churn')[['file', 'added', 'deleted', 'churn']]
        metrics['top_files'] = top_files.to_dict('records')
    
    return metrics

metrics = compute_metrics(commits_df, changes_df)

# Save metrics
with open(data_dir / 'current_metrics.json', 'w') as f:
    json.dump(metrics, f, indent=2)
```

### 2.4 Generate Visualizations
Create 6 high-quality charts:

**Chart 1: Commits Over Time (Time of Day)**
```python
def chart_commits_timeline(commits_df, output_path):
    """Bar chart showing commits by hour of day"""
    if len(commits_df) == 0:
        print("No commits to visualize")
        return
    
    plt.figure(figsize=(12, 7))
    commits_df['hour'] = commits_df['timestamp'].dt.hour
    
    hour_counts = commits_df['hour'].value_counts().sort_index()
    
    ax = plt.subplot(111)
    hour_counts.plot(kind='bar', ax=ax, color='steelblue')
    
    ax.set_xlabel('Hour of Day (UTC)')
    ax.set_ylabel('Number of Commits')
    ax.set_title('Commits by Time of Day (Last 24 Hours)', fontsize=14, fontweight='bold')
    ax.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig(output_path, dpi=300, bbox_inches='tight')
    plt.close()

chart_commits_timeline(commits_df, charts_dir / 'commits_timeline.png')
```

**Chart 2: Changes by Language**
```python
def chart_changes_by_language(changes_df, output_path):
    """Horizontal bar chart of changes by file type"""
    if len(changes_df) == 0:
        print("No changes to visualize")
        return
    
    lang_stats = changes_df.groupby('extension').agg({
        'added': 'sum',
        'deleted': 'sum',
        'churn': 'sum'
    }).sort_values('churn', ascending=True).tail(10)
    
    plt.figure(figsize=(12, 7))
    ax = plt.subplot(111)
    
    y_pos = np.arange(len(lang_stats))
    ax.barh(y_pos, lang_stats['added'], label='Lines Added', color='green', alpha=0.7)
    ax.barh(y_pos, -lang_stats['deleted'], label='Lines Deleted', color='red', alpha=0.7)
    
    ax.set_yticks(y_pos)
    ax.set_yticklabels([ext if ext else '(no ext)' for ext in lang_stats.index])
    ax.set_xlabel('Lines of Code')
    ax.set_title('Code Changes by File Type (Last 24 Hours)', fontsize=14, fontweight='bold')
    ax.legend()
    ax.axvline(x=0, color='black', linewidth=0.8)
    ax.grid(True, alpha=0.3, axis='x')
    
    plt.tight_layout()
    plt.savefig(output_path, dpi=300, bbox_inches='tight')
    plt.close()

chart_changes_by_language(changes_df, charts_dir / 'changes_by_language.png')
```

**Chart 3: Top Files by Churn**
```python
def chart_top_files_churn(changes_df, output_path):
    """Bar chart of most changed files"""
    if len(changes_df) == 0:
        print("No changes to visualize")
        return
    
    top_files = changes_df.groupby('file')['churn'].sum().nlargest(10).sort_values()
    
    plt.figure(figsize=(12, 7))
    ax = plt.subplot(111)
    
    # Truncate long filenames for display
    labels = [f.split('/')[-1][:30] + '...' if len(f) > 30 else f.split('/')[-1] 
              for f in top_files.index]
    
    ax.barh(range(len(top_files)), top_files.values, color='coral')
    ax.set_yticks(range(len(top_files)))
    ax.set_yticklabels(labels)
    ax.set_xlabel('Total Churn (Lines Added + Deleted)')
    ax.set_title('Top 10 Files by Code Churn (Last 24 Hours)', fontsize=14, fontweight='bold')
    ax.grid(True, alpha=0.3, axis='x')
    
    plt.tight_layout()
    plt.savefig(output_path, dpi=300, bbox_inches='tight')
    plt.close()

chart_top_files_churn(changes_df, charts_dir / 'top_files_churn.png')
```

**Chart 4: Contributor Activity**
```python
def chart_contributor_activity(commits_df, changes_df, output_path):
    """Bar chart showing commits and lines changed per contributor"""
    if len(commits_df) == 0:
        print("No commits to visualize")
        return
    
    # Merge commits with changes to get author-based metrics
    if len(changes_df) > 0:
        commit_authors = commits_df[['sha', 'author']].set_index('sha')
        changes_with_author = changes_df.merge(
            commit_authors, left_on='commit', right_index=True, how='left'
        )
        
        contributor_stats = changes_with_author.groupby('author').agg({
            'churn': 'sum',
            'commit': 'nunique'
        }).sort_values('churn', ascending=True).tail(10)
    else:
        contributor_stats = commits_df.groupby('author').size().to_frame('commits')
        contributor_stats.columns = ['commits']
        contributor_stats = contributor_stats.sort_values('commits', ascending=True).tail(10)
    
    plt.figure(figsize=(12, 7))
    ax = plt.subplot(111)
    
    if 'churn' in contributor_stats.columns:
        ax.barh(range(len(contributor_stats)), contributor_stats['churn'], color='mediumseagreen')
        ax.set_xlabel('Total Code Churn (Lines)')
    else:
        ax.barh(range(len(contributor_stats)), contributor_stats['commits'], color='mediumseagreen')
        ax.set_xlabel('Number of Commits')
    
    ax.set_yticks(range(len(contributor_stats)))
    ax.set_yticklabels(contributor_stats.index)
    ax.set_title('Top Contributors by Activity (Last 24 Hours)', fontsize=14, fontweight='bold')
    ax.grid(True, alpha=0.3, axis='x')
    
    plt.tight_layout()
    plt.savefig(output_path, dpi=300, bbox_inches='tight')
    plt.close()

chart_contributor_activity(commits_df, changes_df, charts_dir / 'contributor_activity.png')
```

**Chart 5: Add vs Delete Distribution**
```python
def chart_add_delete_distribution(changes_df, output_path):
    """Scatter plot showing add/delete patterns"""
    if len(changes_df) == 0:
        print("No changes to visualize")
        return
    
    plt.figure(figsize=(12, 7))
    ax = plt.subplot(111)
    
    # Aggregate by file
    file_stats = changes_df.groupby('file').agg({
        'added': 'sum',
        'deleted': 'sum'
    })
    
    ax.scatter(file_stats['deleted'], file_stats['added'], 
               alpha=0.6, s=100, c='purple', edgecolors='black', linewidth=0.5)
    
    # Add diagonal line (equal add/delete)
    max_val = max(file_stats['added'].max(), file_stats['deleted'].max())
    ax.plot([0, max_val], [0, max_val], 'r--', alpha=0.5, label='Equal Add/Delete')
    
    ax.set_xlabel('Lines Deleted')
    ax.set_ylabel('Lines Added')
    ax.set_title('Add vs Delete Distribution per File (Last 24 Hours)', fontsize=14, fontweight='bold')
    ax.grid(True, alpha=0.3)
    ax.legend()
    
    plt.tight_layout()
    plt.savefig(output_path, dpi=300, bbox_inches='tight')
    plt.close()

chart_add_delete_distribution(changes_df, charts_dir / 'add_delete_distribution.png')
```

**Chart 6: Historical Trend (if available)**
```python
def chart_historical_trend(history_file, output_path):
    """Line chart showing trends over time from repo-memory"""
    if not Path(history_file).exists():
        print("No historical data available yet")
        return
    
    # Load historical data
    historical_data = []
    with open(history_file, 'r') as f:
        for line in f:
            if line.strip():
                historical_data.append(json.loads(line))
    
    if len(historical_data) < 2:
        print("Not enough historical data for trends")
        return
    
    df = pd.DataFrame(historical_data)
    df['date'] = pd.to_datetime(df['date'])
    df = df.sort_values('date')
    
    plt.figure(figsize=(12, 7))
    ax = plt.subplot(111)
    
    ax.plot(df['date'], df['total_commits'], marker='o', label='Commits', linewidth=2)
    ax.plot(df['date'], df['total_churn'] / 100, marker='s', label='Churn (√∑100)', linewidth=2)
    ax.plot(df['date'], df['files_changed'], marker='^', label='Files Changed', linewidth=2)
    
    ax.set_xlabel('Date')
    ax.set_ylabel('Count')
    ax.set_title('Historical Code Change Trends (Last 30 Days)', fontsize=14, fontweight='bold')
    ax.legend()
    ax.grid(True, alpha=0.3)
    plt.xticks(rotation=45)
    
    plt.tight_layout()
    plt.savefig(output_path, dpi=300, bbox_inches='tight')
    plt.close()

chart_historical_trend('/tmp/gh-aw/repo-memory/default/history.jsonl', 
                       charts_dir / 'historical_trends.png')
```

### 2.5 Run Python Analysis
Execute the Python script:

```bash
cd /tmp/gh-aw/python
python3 analyze_changes.py
```

## Phase 3: Store Historical Data

### 3.1 Update Repo Memory
Append current metrics to historical tracking:

```bash
# Ensure directory exists
mkdir -p /tmp/gh-aw/repo-memory/default

# Append to history (JSON Lines format - one line per day)
cat /tmp/gh-aw/python/data/current_metrics.json | jq -c '.' >> /tmp/gh-aw/repo-memory/default/history.jsonl

# Keep only last 30 days
tail -30 /tmp/gh-aw/repo-memory/default/history.jsonl > /tmp/gh-aw/repo-memory/default/history.jsonl.tmp
mv /tmp/gh-aw/repo-memory/default/history.jsonl.tmp /tmp/gh-aw/repo-memory/default/history.jsonl
```

## Phase 4: Upload Visualizations

Upload all generated charts as GitHub release assets:

1. Upload each PNG file from `/tmp/gh-aw/python/charts/` using the `upload asset` safe-output tool
2. Collect the URLs returned for each chart
3. Prepare for embedding in the discussion report

Expected charts:
- `commits_timeline.png`
- `changes_by_language.png`
- `top_files_churn.png`
- `contributor_activity.png`
- `add_delete_distribution.png`
- `historical_trends.png` (if historical data exists)

## Phase 5: Generate Discussion Report

Create a comprehensive discussion report with the following structure:

**Title**: `Daily Code Changes Report - [YYYY-MM-DD]`

**Body**:

```markdown
# üìä Daily Code Changes Report

**Period**: Last 24 hours  
**Generated**: [timestamp]  
**Repository**: ${{ github.repository }}

## Executive Summary

[2-3 paragraph summary highlighting:
- Total commits and contributors
- Overall code churn (lines added/deleted)
- Most active files and languages
- Notable patterns or concerns
- Comparison to historical averages if available]

## üìà Visualizations

### Commit Activity Timeline
![Commits Timeline](URL_FROM_UPLOAD_1)

Analysis: [Describe when commits were made, peak hours, distribution patterns]

### Changes by Programming Language
![Changes by Language](URL_FROM_UPLOAD_2)

Analysis: [Which languages saw the most changes, ratios of add/delete per language]

### Top Files by Code Churn
![Top Files Churn](URL_FROM_UPLOAD_3)

Analysis: [Identify most volatile files, potential refactoring candidates or active feature areas]

### Contributor Activity
![Contributor Activity](URL_FROM_UPLOAD_4)

Analysis: [Who contributed most, collaboration patterns, workload distribution]

### Add vs Delete Distribution
![Add Delete Distribution](URL_FROM_UPLOAD_5)

Analysis: [Overall code growth vs reduction, refactoring patterns, file modification patterns]

### Historical Trends (30 Days)
![Historical Trends](URL_FROM_UPLOAD_6)

Analysis: [Long-term trends, weekly patterns, anomalies, overall trajectory]
*Note: Only shown after multiple days of data collection*

<details>
<summary><b>üìã Detailed Metrics</b></summary>

## Commit Statistics

| Metric | Value | Change (7d avg) |
|--------|-------|-----------------|
| Total Commits | [N] | [+/- X%] |
| Contributors | [N] | [+/- X%] |
| Files Changed | [N] | [+/- X%] |
| Lines Added | [+N] | [+/- X%] |
| Lines Deleted | [-N] | [+/- X%] |
| Net Change | [¬±N] | [+/- X%] |
| Total Churn | [N] | [+/- X%] |

## Changes by Language

| Language/Extension | Files | Lines Added | Lines Deleted | Net Change | Churn |
|-------------------|-------|-------------|---------------|------------|-------|
| .go | [N] | [+N] | [-N] | [¬±N] | [N] |
| .js | [N] | [+N] | [-N] | [¬±N] | [N] |
| .md | [N] | [+N] | [-N] | [¬±N] | [N] |
| ... | ... | ... | ... | ... | ... |

## Top 10 Files by Churn

| File | Lines Added | Lines Deleted | Total Churn |
|------|-------------|---------------|-------------|
| path/to/file1.go | [+N] | [-N] | [N] |
| path/to/file2.js | [+N] | [-N] | [N] |
| ... | ... | ... | ... |

## Contributor Breakdown

| Author | Commits | Files Changed | Lines Changed |
|--------|---------|---------------|---------------|
| Author1 | [N] | [N] | [¬±N] |
| Author2 | [N] | [N] | [¬±N] |
| ... | ... | ... | ... |

## Commit Messages Summary

Top themes/patterns in commit messages:
- [Theme 1]: [count] commits
- [Theme 2]: [count] commits
- [Theme 3]: [count] commits

</details>

## üí° Insights & Recommendations

1. **[Insight 1]**: [Specific observation based on data]
   - Recommendation: [Actionable suggestion]

2. **[Insight 2]**: [Another observation]
   - Recommendation: [Actionable suggestion]

3. **[Insight 3]**: [Pattern identified]
   - Recommendation: [Actionable suggestion]

## üîç Quality Indicators

- **Code Growth Rate**: [Description of net change trend]
- **Refactoring Activity**: [Ratio of add/delete, areas with high churn]
- **Collaboration Health**: [Contributor distribution, commit frequency]
- **Focus Areas**: [Most active directories/modules]

---

*This report was automatically generated using Python data science tools (pandas, numpy, matplotlib, seaborn).*  
*Historical tracking: [N] days of data | Next report: [tomorrow's date]*  
*Workflow Run: https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }}*
```

## Report Guidelines

- **Use h3 (###) or lower** for all headers to maintain document hierarchy (discussion title is h1)
- **Embed all charts** as images using uploaded asset URLs
- **Provide analysis** for each visualization - don't just show charts
- **Use collapsible details** for detailed metrics tables to keep report scannable
- **Include trend indicators** (‚¨ÜÔ∏è/‚û°Ô∏è/‚¨áÔ∏è) when historical data is available
- **Make it actionable** - provide specific insights and recommendations
- **Handle edge cases**: 
  - If no commits in 24h, report "No activity" with historical context
  - If historical data unavailable, note "Building trend data - check back tomorrow"
  - If data collection fails, provide debugging information

## Best Practices

1. **Efficient Git Operations**: Use `--since="24 hours ago"` to limit scope
2. **Handle Empty Data**: Always check if DataFrames are empty before plotting
3. **Quality Charts**: DPI 300, clear labels, consistent styling
4. **Error Handling**: Wrap Python in try-except, log errors clearly
5. **Data Persistence**: Always append to repo-memory for trend tracking
6. **Performance**: Complete analysis in under 30 minutes
7. **Security**: Never execute code files, only analyze git metadata

## Success Criteria

- ‚úÖ All git data collected successfully
- ‚úÖ Python analysis completes without errors
- ‚úÖ 5-6 high-quality charts generated (6th only if historical data exists)
- ‚úÖ All charts uploaded as assets
- ‚úÖ Metrics appended to repo-memory
- ‚úÖ Discussion created with embedded visualizations
- ‚úÖ Report is comprehensive yet scannable
- ‚úÖ Insights are data-driven and actionable
